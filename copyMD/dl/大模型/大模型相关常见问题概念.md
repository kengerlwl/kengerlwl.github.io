---
title: 大模型相关常见问题概念
top: false
cover: false
toc: true
mathjax: true
date: 2024-07-23 15:27:31
password:
summary:
tags:
- pytorch
- 推理
- LLM
categories:
- 学术
---



# 背景

有些东西，名词，经常听见，也大概知道是什么意思，但是就是总记不住，列个case。



# 幻觉

**如果AI模型所生成输出没有任何已知事实的支持，幻觉便会发生。**产生与训练数据不一致或与输入无关的输出的模型。

简而言之：一本正经，胡说八道

包括RAG在内的自回归模型中幻觉的不可避免性可以最小化，但不能完全根除。



# RAG相关

## 向量数据库

### HNSW算法

HNSW（Hierarchical Navigable Small World）算法是一种用于**高效近似最近邻搜索**（Approximate Nearest Neighbor Search, ANNS）的图结构算法。它在处理高维数据时表现出色，广泛应用于图像检索、推荐系统、自然语言处理等领域。HNSW算法的核心思想是通过构建一个分层的小世界图（Small World Graph），**使得在高维空间中进行近似最近邻搜索变得更加高效。**

**HNSW算法的基本原理**

1. **分层结构**：HNSW构建了一个多层次的图结构，每一层都是一个小世界图。顶层的图节点较少，底层的图节点较多。每一层的图都是前一层的子集，顶层的图节点是底层图节点的超集。
2. **小世界图**：每一层的图都是一个小世界图，具有较短的平均路径长度和较高的聚类系数。小世界图的特性使得在图中进行搜索时，可以快速接近目标节点。
3. **导航机制**：搜索过程从顶层开始，逐层向下进行。在每一层中，算法会从当前节点的邻居中选择最接近目标节点的节点，直到到达底层。在底层进行精确的近邻搜索。



## 多路召回重排

### RRF

### RF的基本原理

**倒数排序融合（RRF）**是基于倒数排名的概念。其核心思想是，一个项目在各个源中的排名的倒数可以被累加，从而得到一个综合分数，用于最终的排名。RRF的公式如下：

![img](https://cdn.jsdelivr.net/gh/kengerlwl/kengerlwl.github.io/image/b097c8b7700f2508ebb1f27f6bab4d5b/b5120cd3566f65794b810a7ef13c03b0.png)



- 其中，**rank*i* 是一个项目在第 *i* 个源中的排名。**（倒数可以让排名越高，得到的分数越高）
- *k* 是源的数量。
- *c* 是一个**常数，通常设为60，用于减轻排名靠后的项目的权重。**





# Chat 和 Completion 的区别



OpenAI的ChatCompletion和Completion都是自然语言生成模型的接口，但它们的用途和应用场景略有不同。

|          | ChatCompletion                                               | Completion                                                   |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 区别     | 专为生成**对话和聊天**场景而设计。                           | 是一个**通用**的自然语言生成接口，支持生成各种类型的文本，包括段落、摘要、建议、答案等等。 |
| 适用场景 | ChatCompletion接口生成的文本通常会更具有人类对话的风格和语调，可以用于智能客服、聊天机器人等场景，以及在日常聊天中帮助用户自动生成回复。 | **Completion接口的输出更为多样化，可能会更加严谨和专业，适用于各种文本生成场景，例如文章创作、信息提取、机器翻译、自然语言问题回答等等。** |

本质上，是类似的，都是通过构造prompt输入给大模型，只是输入的方式不一样，一个是直接总结好所有的背景，要求输入给大模型，一个是把历史对话记录输入给大模型。



## ChatCompletion

```python
# Note: you need to be using OpenAI Python v0.27.0 for the code below to work
import openai

openai.ChatCompletion.create(
  model="gpt-3.5-turbo",
  messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Who won the world series in 2020?"},
        {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
        {"role": "user", "content": "Where was it played?"}
    ]
)
```

messages必须是消息对象的数组，其中每个对象都有一个角色（“system”、“user”、“assistant”）和内容（消息的内容）。对话可以短至 1 条消息或填满许多页面。

通常，对话的格式首先是system消息，然后是交替的user和assistant消息。

- system消息有助于设置聊天AI助手的行为。在上面的例子中，被指示“你是一个乐于助人的助手”。
- user消息有助于指示助手。它们可以由应用程序的最终用户生成，也可以由开发人员设置为指令。
- assistant消息有助于存储先前的响应。它们**也可以由开发人员编写，以帮助提供所需行为的示例。**









# 微调

## RLHF

## MOE

![image-20240814164819635](C:\Users\rainwlliu\AppData\Roaming\Typora\typora-user-images\image-20240814164819635.png)

### MoE结构大模型

1. **多专家网络**：MoE模型包含多个专家网络，每个专家网络可以是一个独立的神经网络。专家网络的数量可以非常多，从几十到上千不等。
2. **门控机制**：**MoE模型使用一个门控网络（Gate）来决定在每次推理时激活哪些专家。门控网络根据输入数据的特征，选择最适合处理该数据的专家。**
3. **稀疏激活**：在每次推理过程中，MoE模型只激活一小部分专家（例如，2-4个），而不是所有专家。这种稀疏激活机制大大减少了计算量。
4. **高效计算**：由于只激活少量专家，MoE模型在保持高表达能力的同时，显著降低了计算成本。

### 对比普通模型

1. **计算效率**：
   - **MoE模型**：通过稀疏激活机制，**只计算少量专家的输出，计算效率更高。**
   - **普通模型**：所有参数和层都参与计算，计算量较大。
2. **模型容量**：
   - **MoE模型**：可以包含大量专家网络，**总参数量可以非常大，但每次推理只使用一部分参数。**
   - **普通模型**：参数量固定，所有参数都参与计算，模型容量受限于计算资源。



**占用的显存还是变大了，只是计算速度确实很快，且性能大大提高了。**

Mixtral 8x7B同样是一个Decoder Only的模型，区别于传统的LLaMA等模型，FNN层由8个前馈神经网络（Expert）组成。如果我们从某一个Token的视角看，这个Token会经过其中的2个前馈神经网络或者说Expert。也就是说虽然**整个模型的参数量是46B，但是在推理过程中激活的参数只有13B。**

虽然Mixtral 8x7B在推理过程中同一时间激活的参数只有13B左右，**但是为了保证推理性能，还是需要将全部参数（46B）读入显存**，以A100-80GB为例，对于46B的参数的模型，按照FP16精度来估算，参数预期占用92GB显存。以batch为20、Context length=1024、Generate length=1024来看、KV Cache需要的显存为20GB**[2]** ，也就是说理想态下（不考虑显存碎片，Activate output），**至少需要2张A100-80GB完成推理**。



# 量化与精度

量化是大模型压缩方法的一种，

常见的精度。

| dtype     | 每10亿参数需要占内存 |
| --------- | -------------------- |
| float32   | 4G                   |
| fp16/bf16 | 2G                   |
| int8      | 1G                   |
| int4      | 0.5G                 |

## fp16/bf16 的区别

两个都是16位的浮点数，但是其中整数位和浮点数的数量占比不一样。

![16](https://raw.githubusercontent.com/TransformersWsz/picx-images-hosting/master/image.1hs0v083jb.png)



二者都是占用16bit空间。

- FP16由1个符号位、5**个指数位和10个尾数位组成**。FP16在表达小数时具有较高的精度，但表示的最大范围相对BF16比较小。相比BF16，在表达较大的数时更容易出现上溢的情况。
- BF16由1个符号位、**8个指数位和7个尾数位组成**。相比于FP16，BF16牺牲了一些尾数位以增加指数位，扩大了表达的范围，但是精度降低了，因此对于对精度需求比较高的模型，模型可能效果不如FP16。

模型训练时使用BF16和FP16都可以降低内存使用和传输量，提高训练效率。



# 模型格式



## GGUF

**GGUF文件全称是GPT-Generated Unified Format，是由Georgi Gerganov定义发布的一种大模型文件格式。Georgi Gerganov是著名开源项目llama.cpp的创始人。**

**GGUF就是一种二进制格式文件的规范，原始的大模型预训练结果经过转换后变成GGUF格式可以更快地被载入使用，也会消耗更低的资源**。

经常是量化后的大模型







# ref



[大型语言模型的幻觉问题_大语言模型幻觉问题-CSDN博客](https://blog.csdn.net/wdnshadow/article/details/135433235?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-135433235-blog-134389457.235^v43^pc_blog_bottom_relevance_base2&spm=1001.2101.3001.4242.2&utm_relevant_index=2)

[Reciprocal Rank Fusion (RRF) – 我的AI](https://www.aithinkings.com.cn/?p=76)

[MOE模型的详解     太初（无锡）电子科技有限公司](http://www.tecorigin.com/newsdetails/110/77.html)

