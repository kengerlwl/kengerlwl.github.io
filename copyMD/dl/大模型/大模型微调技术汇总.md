---
title: 大模型微调技术汇总
top: false
cover: false
toc: true
mathjax: true
date: 2024-05-28 15:27:31
password:
summary:
tags:
- pytorch
- 微调
- LLM
categories:
- 学术
---


# 介绍

主要介绍各种大模型**微调（Fine tuning）**技术

受GPT论文的影响，目前大模型通用训练模式是三阶段训练模式

- **第一阶段pre-train**

- **第二阶段是SFT**

- **第三阶段是RLHF**

**通过三阶段训练分别得到base模型以及chat模型，chat模型是在base模型基础进行通用任务的SFT以及RLHF，使模型具备了对话能力、推理能力、用户偏好对齐、以及其他的NLU的能力。**





```
模型调参分领域 (Model Tuning Subfields)
  ├── Adaptor-based finetune
  │     ├── Serial Adapter
  │     ├── AdapterFusion
  │     ├── CoDA
  │     ├── Parallel Adapter
  ├── Soft Prompt-based finetune
  │     ├── Prefix-tuning
  │     ├── prompt-tuning
  │     ├── p-tuning v2
  │     ├── p-tuning
  │     ├── APrompt
  ├── Low-rank Decomposition
  │     ├── LoRA
  │     ├── DoRA
  │     ├── KronA
  │     ├── KAdaptation
  ├── LoRA Derivatives
        ├── AdaLoRA
        ├── DyLoRA
        ├── AutoLoRA
        ├── Laplace-LoRA
        ├── MOELoRA

模型推理 (Model Inference)
  ├── Post-training Quantization
  │     ├── ZeroQuant
  │     ├── SmoothQuant
  │     ├── LLMint8()
  │     ├── GPTQ
  │     ├── AWQ
  │     ├── GPTQ
  │     ├── ZeroQuant-V2
  │     ├── RPTQ
  │     ├── LLM-QAT
  ├── Quantization-Aware Training
  │     ├── PEQA
  │     ├── QLORA
  ├── Pruning
  │     ├── SparseGPT
  │     ├── Wanda
  │     ├── LoRAPrune
  │     ├── GUM
  │     ├── LLM-Pruner
  ├── Knowledge Distillation
  │     ├── Black-box KD
  │     ├── White-box KD
  ├── Compact Design
  │     ├── Sparse Attention
  │     ├── Linear Approximate Attention
  ├── Dynamic Networks
  │     ├── Mixture of Experts
  │     ├── Model Compression
  ├── Accelerator Framework


智能体决策图谱 (Intelligent Agent Decision Map)
  ├── 模型对齐 (Model Alignment)
  │     ├── RL-Based Method
  │     │     ├── RLHF
  │     │     ├── f-DPG
  │     │     ├── SENSEI
  │     │     ├── RLSF
  │     │     ├── CoH
  │     ├── SL-Based Method
  │     │     ├── RAFT
  │     │     ├── UMA
  │     ├── Task Decomposition
  │     │     ├── Factored Cognition
  │     │     ├── Iterated Distillation and Amplification (DA)
  │     │     ├── Recursive Reward Modeling (RRM)
  │     │     ├── Sandwitching
  │     │     ├── Process Supervision
  │     ├── Constitutional AI
  │           ├── CAI
  │           ├── Dromedary

```









# 正文

## 微调方式分类

- SFT，有监督微调







## 微调参数上分类

### 全微调（Full Fine-tuning）

**全微调是指对整个预训练模型进行微调，包括所有的模型参数**。在这种方法中，预训练模型的所有层和参数都会被更新和优化，以适应目标任务的需求。**这种微调方法通常适用于任务和预训练模型之间存在较大差异的情况，或者任务需要模型具有高度灵活性和自适应能力的情况**。Full Fine-tuning需要较大的计算资源和时间，但可以获得更好的性能。

- 微调所有层：将预训练模型的所有层都参与微调，以适应新的任务。

### 部分微调（Repurposing）

部分微调是指在微调过程中只更新模型的顶层或少数几层，而保持预训练模型的底层参数不变。

- 微调顶层：只微调预训练模型的顶层，以适应新的任务。
- 冻结底层：将预训练模型的底层固定不变，只对顶层进行微调。
- 逐层微调：从底层开始，逐层微调预训练模型，直到所有层都被微调。
- 迁移学习：将预训练模型的知识迁移到新的任务中，以提高模型性能。这种方法通常使用微调顶层或冻结底层的方法。

# PEFT

**PEFT（Parameter-Efficient Fine-Tuning）**是hugging face开源的一个参数高效微调大模型的工具，里面集成了4种微调大模型的方法，可以**通过微调少量参数就达到接近微调全量参数的效果**，使得在GPU资源不足的情况下也可以微调大模型。



## Prefix-Tuning

**Optimizing Continuous Prompts for Generation**

在Prompt思想的启发下，在Prefix-Tuning中提出了给每一个input输入增加一个连续的任务相关的embedding向量(`continuous task-specific vectors`)来进行训练。

![img](https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/0f4046b664d5122350fcbbac0ba05bfe/65ef95b6fb2b3d4289081022e5346ae6.jpeg)

这里的连续(`continuous`)是相对于人工定义的文本prompt token的离散(`discrete`)来说的，例如一个人工定义的prompt token数组是`['The', 'movie', 'is', '[MASK]']`，**把其中的token `The`替换成一个embedding向量做为输入的话，其中embedding就是连续的(`continuous`)表达方式。在下游任务重训的时候固定原有的大模型所有参数，只用重训跟下游任务相关的前缀向量(prefix embedding)即可。**

**对于自回归的LM模型(例如`GPT-2`)来说，会在原有prompt之前增加prefix(`z = [PREFIX; x; y]`);**对于encoder+decoder的LM模型(例如`BART`)来说，**会分别在encoder和decoder的输入前加上prefix(`z = [PREFIX; x; PREFIX'; y],`)**。如下图所示，`P_idx`表示加的前缀序列, `h`对应的是可学习的参数， 用`Pθ=[h1, h2, h3, ...]`表示可学习参数矩阵。

![img](https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/0f4046b664d5122350fcbbac0ba05bfe/a4698b59ff406559b283ec169736cf24.jpeg)

直接学习参数效果不好，所以使用MLP网络对`Pθ`进行了reparameter修正，即`Pθ[i,:] = MLP(Pθ'[i,:])`，重训完只用保存prefix的`Pθ`相关参数。



## P-Tuning v1：GPT Understands, Too



P-Tuning这个词在这篇文中被第一次提出，为了跟后续P-Tuning v2区分，这里加了个v1。本论文主要是为了解决GPT大模型在自然语言理解任务([NLU, Nature Language Understanding](https://en.wikipedia.org/wiki/Natural-language_understanding) 重训效果不好的问题。在P-Tuning方法中会在连续向量空间中自动搜索合适的prompt，来增强重训练的效果。

对于之前存在的离散prompt搜索方法(discrete prompt search)来说, 比如[AUTOPROMPT](https://arxiv.org/pdf/2010.15980.pdf)、[LPAQA](https://aclanthology.org/2020.tacl-1.28.pdf), 其中的`Prompt Generator`通过接受离散的反馈来选择合适的prompt。

![img](https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/0f4046b664d5122350fcbbac0ba05bfe/ba4e2728c47760331ecb0f94003ac7cb.jpeg)

## Parameter-Efficient Prompt Tuning（Prompt Tuning）

本篇论文可以看成是prefix-tuning的简化版，一方面文中实验证明了使用自动生成的`soft prompt`方法进行tuning的效果跟`model tuning`差不多，同时超过了人工设计的prompt。

![img](https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/0f4046b664d5122350fcbbac0ba05bfe/79b4b50829e3789c86b59cb8de65cf7a.jpeg)





## P-Tuning v2

**在模型参数量小于10B的训练中，prompt tuning效果还是不及FT(fine-tuning), `P-Tuning v2`支持`330M~10B`规模的多任务tuning**。`P-Tuning v2`可以看成是[Deep Prompt Tuning](https://arxiv.org/pdf/2004.04906.pdf)在NLU领域的实现，而`Deep Prompt Tuning`用于问答任务的训练。

跟之前区别主要有以下几点：

1. 对于NLU任务没有使用像MLP的Reparameterization。
2. 在模型的每一层上都加上了layer prompt，不同任务可以共享相同的网络参数，支持多任务学习
3. 在分类头的verbalizer中使用了一个随机初始化的`linear head`
4. Prompt长度对于简单分类任务小于20，对于像序列标注这样的复杂任务需要100左右



## Adapter-based Methods（基于适配器的方法）:

《Parameter-Efficient Transfer Learning for NLP》提出针对 BERT 的 PEFT微调方式，拉开了 PEFT 研究的序幕。他们指出，在面对特定的下游任务时，如果进行 Full-Fintuning（即预训练模型中的所有参数都进行微调），太过低效；而如果采用固定预训练模型的某些层，只微调接近下游任务的那几层参数，又难以达到较好的效果。

**在训练时，固定住原来预训练模型的参数不变，只对新增的 Adapter 结构进行微调。**同时为了保证训练的高效性（也就是尽可能少的引入更多参数），他们将 Adapter 设计为这样的结构：

> 首先是一个 **down-project 层将高维度特征映射到低维特征；然后过一个非线形层之后，再用一个 up-project 结构将低维特征映射回原来的高维特征**；同时也设计了 skip-connection 结构，确保了在最差的情况下能够退化为identity（类似残差结构）。

![img](https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/0f4046b664d5122350fcbbac0ba05bfe/d542958f47a930d26d048415a69c083b.jpg)

### Prompt-based Methods（基于提示的方法）:

这个分支侧重于使用连续的提示（如嵌入向量）来调整模型的行为，而不是直接修改模型的权重。这类方法通常用于生成任务，例如文本生成。提示可以视为模型输入的一部分，它们会被训练以激发模型生成特定的输出。示例包括Prefix-tuning、Prompt tuning等，参加上文介绍。



### Low-rank Adaptation（低秩适配）:

低秩适配方法**致力于将模型权重的改变限制在一个低秩子空间内**。这通常涉及对模型的权重矩阵进行分解，只微调其中的一小部分参数。这样可以有效减少计算资源的消耗，同时仍然允许模型有足够的灵活性来学习新任务。**LoRA和它的变种，如Q-LoRA、Delta-LoRA、LoRA-FA等，都属于这个类别。**





# TO DO

- [【Prompt系列】(二) [论文分享] AutoPrompt：别瞎设计了，费力不讨好 - 掘金](https://juejin.cn/post/7062738936673763364)
- 



# ref

[详解大模型微调方法Prompt Tuning(内附实现代码) | by MLTalks | Medium](https://mltalks.medium.com/%E8%AF%A6%E8%A7%A3%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95prompt-tuning-%E5%86%85%E9%99%84%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81-7e4276927729)

[LLM-SFT-trick - 知乎](https://zhuanlan.zhihu.com/p/682604566)
