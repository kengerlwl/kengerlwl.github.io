<!DOCTYPE html>
<html lang="zh-CN">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <title>大模型底层八股 - Kenger`s Blog</title><meta name="Description" content="Hugo theme - LoveIt"><meta property="og:url" content="https://kengerlwl.github.io/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E5%85%AB%E8%82%A1/">
  <meta property="og:site_name" content="Kenger`s Blog">
  <meta property="og:title" content="大模型底层八股">
  <meta property="og:description" content="Transformer八股 为什么在Transformer模型中使用Layer Normalization（Layer Norm）而不是Batch">
  <meta property="og:locale" content="zh_CN">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-08-18T15:27:31+00:00">
    <meta property="article:modified_time" content="2024-08-18T15:27:31+00:00">
    <meta property="article:tag" content="Pytorch">
    <meta property="article:tag" content="Transformer">
    <meta property="article:tag" content="Llm">
    <meta property="og:image" content="https://kengerlwl.github.io/logo.png">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://kengerlwl.github.io/logo.png">
  <meta name="twitter:title" content="大模型底层八股">
  <meta name="twitter:description" content="Transformer八股 为什么在Transformer模型中使用Layer Normalization（Layer Norm）而不是Batch">
<meta name="application-name" content="LoveIt">
<meta name="apple-mobile-web-app-title" content="LoveIt"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://kengerlwl.github.io/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E5%85%AB%E8%82%A1/" /><link rel="prev" href="https://kengerlwl.github.io/%E5%A4%A7%E6%A8%A1%E5%9E%8Btoken%E5%8E%8B%E7%BC%A9/" /><link rel="next" href="https://kengerlwl.github.io/python%E5%85%AB%E8%82%A1%E9%A2%98/" /><link rel="stylesheet" href="/css/style.min.css"><link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css"></noscript><link rel="preload" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
        <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css"></noscript><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "BlogPosting",
        "headline": "大模型底层八股",
        "inLanguage": "zh-CN",
        "mainEntityOfPage": {
            "@type": "WebPage",
            "@id": "https:\/\/kengerlwl.github.io\/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E5%85%AB%E8%82%A1\/"
        },"image": [{
                            "@type": "ImageObject",
                            "url": "https:\/\/kengerlwl.github.io\/images\/4.jpg",
                            "width":  1280 ,
                            "height":  720 
                        }],"genre": "posts","keywords": "pytorch, Transformer, LLM","wordcount":  4478 ,
        "url": "https:\/\/kengerlwl.github.io\/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E5%85%AB%E8%82%A1\/","datePublished": "2024-08-18T15:27:31+00:00","dateModified": "2024-08-18T15:27:31+00:00","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","publisher": {
            "@type": "Organization",
            "name": "kenger","logo": {
                    "@type": "ImageObject",
                    "url": "https:\/\/kengerlwl.github.io\/images\/avatar.png",
                    "width":  1080 ,
                    "height":  1080 
                }},"author": {
                "@type": "Person",
                "name": "kenger"
            },"description": ""
    }
    </script></head>
    <body data-header-desktop="fixed" data-header-mobile="auto"><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Kenger`s Blog"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="https://kengerlwl.github.io/favicon-16x16.ico"
        data-srcset="https://kengerlwl.github.io/favicon-16x16.ico, https://kengerlwl.github.io/favicon-16x16.ico 1.5x, https://kengerlwl.github.io/favicon-16x16.ico 2x"
        data-sizes="auto"
        alt="https://kengerlwl.github.io/favicon-16x16.ico"
        title="https://kengerlwl.github.io/favicon-16x16.ico" /><span class="header-title-pre"> <i class='far' aria-hidden='true'></i></span>Kenger`s Blog</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> 所有文章 </a><a class="menu-item" href="/tags/"> 标签 </a><a class="menu-item" href="/categories/"> 分类 </a><a class="menu-item" href="/about/"> 关于 </a><a class="menu-item" href="https://github.com/kengerlwl/kengerlwl.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i>  </a><span class="menu-item delimiter"></span><span class="menu-item search" id="search-desktop">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-desktop">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-desktop" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-desktop" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-desktop">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </span><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                    <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
                </a><a href="javascript:void(0);" class="menu-item language" title="选择语言">
                    <i class="fa fa-globe" aria-hidden="true"></i>                      
                    <select class="language-select" id="language-select-desktop" onchange="location = this.value;"><option value="/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E5%85%AB%E8%82%A1/" selected>简体中文</option></select>
                </a></div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Kenger`s Blog"><img
        class="lazyload logo"
        src="/svg/loading.min.svg"
        data-src="https://kengerlwl.github.io/favicon-16x16.ico"
        data-srcset="https://kengerlwl.github.io/favicon-16x16.ico, https://kengerlwl.github.io/favicon-16x16.ico 1.5x, https://kengerlwl.github.io/favicon-16x16.ico 2x"
        data-sizes="auto"
        alt="https://kengerlwl.github.io/favicon-16x16.ico"
        title="https://kengerlwl.github.io/favicon-16x16.ico" /><span class="header-title-pre"> <i class='far' aria-hidden='true'></i></span>Kenger`s Blog</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><div class="search-wrapper">
                    <div class="search mobile" id="search-mobile">
                        <input type="text" placeholder="搜索文章标题或内容..." id="search-input-mobile">
                        <a href="javascript:void(0);" class="search-button search-toggle" id="search-toggle-mobile" title="搜索">
                            <i class="fas fa-search fa-fw" aria-hidden="true"></i>
                        </a>
                        <a href="javascript:void(0);" class="search-button search-clear" id="search-clear-mobile" title="清空">
                            <i class="fas fa-times-circle fa-fw" aria-hidden="true"></i>
                        </a>
                        <span class="search-button search-loading" id="search-loading-mobile">
                            <i class="fas fa-spinner fa-fw fa-spin" aria-hidden="true"></i>
                        </span>
                    </div>
                    <a href="javascript:void(0);" class="search-cancel" id="search-cancel-mobile">
                        取消
                    </a>
                </div><a class="menu-item" href="/posts/" title="">所有文章</a><a class="menu-item" href="/tags/" title="">标签</a><a class="menu-item" href="/categories/" title="">分类</a><a class="menu-item" href="/about/" title="">关于</a><a class="menu-item" href="https://github.com/kengerlwl/kengerlwl.github.io" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw' aria-hidden='true'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="切换主题">
                <i class="fas fa-adjust fa-fw" aria-hidden="true"></i>
            </a><a href="javascript:void(0);" class="menu-item" title="选择语言">
                    <i class="fa fa-globe fa-fw" aria-hidden="true"></i>
                    <select class="language-select" onchange="location = this.value;"><option value="/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E5%85%AB%E8%82%A1/" selected>简体中文</option></select>
                </a></div>
    </div>
</header><div class="search-dropdown desktop">
        <div id="search-dropdown-desktop"></div>
    </div>
    <div class="search-dropdown mobile">
        <div id="search-dropdown-mobile"></div>
    </div><main class="main">
                <div class="container"><div class="toc" id="toc-auto">
            <h2 class="toc-title">目录</h2>
            <div class="toc-content" id="toc-content-auto"></div>
        </div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">大模型底层八股</h1><div class="post-meta">
            <div class="post-meta-line"><span class="post-author"><a href="https://kengerlwl.github.io/" title="Author" target="_blank" rel="noopener noreffer author" class="author"><i class="fas fa-user-circle fa-fw" aria-hidden="true"></i>kenger</a></span>&nbsp;<span class="post-category">收录于 <a href="/categories/%E5%AD%A6%E6%9C%AF/"><i class="far fa-folder fa-fw" aria-hidden="true"></i>学术</a></span></div>
            <div class="post-meta-line"><i class="far fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;<time datetime="2024-08-18">2024-08-18</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden="true"></i>&nbsp;约 4478 字&nbsp;
                <i class="far fa-clock fa-fw" aria-hidden="true"></i>&nbsp;预计阅读 9 分钟&nbsp;</div>
        </div><div class="details toc" id="toc-static"  data-kept="">
                <div class="details-summary toc-title">
                    <span>目录</span>
                    <span><i class="details-icon fas fa-angle-right" aria-hidden="true"></i></span>
                </div>
                <div class="details-content toc-content" id="toc-content-static"><nav id="TableOfContents">
  <ul>
    <li><a href="#为什么在transformer模型中使用layer-normalizationlayer-norm而不是batch-normalizationbatch-norm"><strong>为什么在Transformer模型中使用Layer Normalization（Layer Norm）而不是Batch Normalization（Batch Norm）</strong></a></li>
    <li><a href="#attention手搓">Attention手搓</a>
      <ul>
        <li><a href="#自注意力机制code-dmoqv同源">自注意力机制code dmo（qv同源）</a></li>
        <li><a href="#multi-head-attention的对比">multi head Attention的对比</a></li>
      </ul>
    </li>
    <li><a href="#multi-query-attention-与-grouped-query-attention-是否了解区别是什么">Multi-query Attention 与 Grouped-query Attention 是否了解？区别是什么？</a></li>
    <li><a href="#几种位置编码">几种位置编码</a>
      <ul>
        <li><a href="#绝对位置编码-absolute-position-encoding">绝对位置编码 Absolute Position Encoding</a>
          <ul>
            <li><a href="#原理">原理</a></li>
            <li><a href="#缺点">缺点</a></li>
          </ul>
        </li>
        <li><a href="#相对位置编码-relative-position-encoding">相对位置编码 Relative Position Encoding</a>
          <ul>
            <li><a href="#原理-1">原理</a></li>
            <li><a href="#优点">优点</a></li>
            <li><a href="#缺点-1">缺点</a></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><a href="#几种ffn前馈神经网络">几种ffn，前馈神经网络</a>
      <ul>
        <li><a href="#为什么需要ffn">为什么需要ffn</a></li>
      </ul>
    </li>
    <li><a href="#混合精度训练">混合精度训练</a>
      <ul>
        <li><a href="#混合精度训练的基本原理">混合精度训练的基本原理</a></li>
      </ul>
    </li>
    <li><a href="#分布式训练dpmpddpppzero的三个stage">分布式训练dp，mp，ddp，pp；zero的三个stage</a></li>
    <li><a href="#多模态clip">多模态clip</a></li>
    <li><a href="#多模态的实现方式双流单流">多模态的实现方式（双流、单流）</a></li>
  </ul>

  <ul>
    <li><a href="#batch上如何优化">batch上如何优化</a></li>
    <li><a href="#预测推理speculative-inference">预测推理（<strong>Speculative inference</strong>）</a></li>
    <li><a href="#prefix-caching-优化">prefix Caching 优化</a></li>
    <li><a href="#kv-cache">KV-Cache</a></li>
    <li><a href="#训练加速---pipeline并行">训练加速&mdash;Pipeline并行</a></li>
    <li><a href="#训练加速---tensor并行相当于每一层直接切分为多个并行">训练加速&mdash;Tensor并行（相当于每一层，直接切分为多个并行）</a></li>
  </ul>
</nav></div>
            </div><div class="content" id="content"><h1 id="transformer八股">Transformer八股</h1>
<h2 id="为什么在transformer模型中使用layer-normalizationlayer-norm而不是batch-normalizationbatch-norm"><strong>为什么在Transformer模型中使用Layer Normalization（Layer Norm）而不是Batch Normalization（Batch Norm）</strong></h2>
<p>Layer Norm和Batch Norm是两种不同的归一化方法，各自适用于不同的场景。<strong>Batch Norm在<a href="https://www.zhihu.com/search?q=%e5%8d%b7%e7%a7%af%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7b%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3575127405%7d" target="_blank" rel="noopener noreffer ">卷积神经网络</a>中得到广泛应用，它通过对同一批次（batch）中不同样本的相同特征进行归一化，来加速训练过程并减少过拟合。然而，在自然语言处理任务中，由于输入序列的长度不一致，并且Batch Norm对批次大小较为敏感，因此并不适合用于Transformer模型。</strong></p>
<p>相比之下，<strong>Layer Norm对每个样本独立进行归一化，使得模型对序列长度和批次大小不敏感，更适合处理<a href="https://www.zhihu.com/search?q=%e5%a4%a7%e5%9e%8b%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7b%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3575127405%7d" target="_blank" rel="noopener noreffer ">大型语言模型</a>任务</strong>。此外，Layer Norm还能够减缓<a href="https://www.zhihu.com/search?q=%e6%a2%af%e5%ba%a6%e6%b6%88%e5%a4%b1%e9%97%ae%e9%a2%98&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7b%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3575127405%7d" target="_blank" rel="noopener noreffer ">梯度消失问题</a>，使得模型在训练过程中更加稳定。因此，在Transformer模型中选择了Layer Norm作为<a href="https://www.zhihu.com/search?q=%e5%bd%92%e4%b8%80%e5%8c%96%e6%96%b9%e6%b3%95&amp;search_source=Entity&amp;hybrid_search_source=Entity&amp;hybrid_search_extra=%7b%22sourceType%22%3A%22answer%22%2C%22sourceId%22%3A3575127405%7d" target="_blank" rel="noopener noreffer ">归一化方法</a>。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/11040cade517023b4c15b886881ac0a1.webp?source=1def8aca"
        data-srcset="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/11040cade517023b4c15b886881ac0a1.webp?source=1def8aca, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/11040cade517023b4c15b886881ac0a1.webp?source=1def8aca 1.5x, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/11040cade517023b4c15b886881ac0a1.webp?source=1def8aca 2x"
        data-sizes="auto"
        alt="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/11040cade517023b4c15b886881ac0a1.webp?source=1def8aca"
        title="img" /></p>
<h2 id="attention手搓">Attention手搓</h2>
<p>Self-attention（自注意力）机制是Transformer模型的核心组成部分，它允许模型在处理序列数据时，为序列中的每个元素（如词或标记）分配不同的注意力权重，从而捕捉序列内的依赖关系。
Self-attention的基本公式如下：</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/931a914b10b561c98a27dd83abd03a91.png"
        data-srcset="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/931a914b10b561c98a27dd83abd03a91.png, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/931a914b10b561c98a27dd83abd03a91.png 1.5x, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/931a914b10b561c98a27dd83abd03a91.png 2x"
        data-sizes="auto"
        alt="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/931a914b10b561c98a27dd83abd03a91.png"
        title="file" /></p>
<ol>
<li><strong>计算Query（Q）、Key（K）和Value（V）：</strong>
这些矩阵是通过将输入序列的嵌入（或隐藏状态）与三个不同的权重矩阵（Wq、Wk、Wv）相乘得到的。这三个权重矩阵是模型需要学习的参数。</li>
</ol>
<ul>
<li>Q = X * Wq</li>
<li>K = X * Wk</li>
<li>V = X * Wv
其中，X是输入序列的嵌入矩阵，维度为，N是序列长度，D是嵌入维度。</li>
</ul>
<ol start="2">
<li><strong>计算注意力得分：</strong>
使用Query和Key计算注意力得分，这反映了序列中每个元素对其他元素的重要性。</li>
</ol>
<ul>
<li>得分 = Q * K^T</li>
</ul>
<ol start="3">
<li><strong>应用softmax函数：</strong>
将得分通过softmax函数转换为概率分布**，确保所有注意力权重的总和为1。**</li>
</ol>
<ul>
<li><strong>概率分布 = softmax(得分 / √D)</strong>
<ul>
<li><strong>为什么要除以根号D</strong>：<strong>除以根号 d的主要原因是为了防止点积结果的数值过大，从而导致Softmax函数的梯度消失问题。</strong></li>
</ul>
</li>
</ul>
<ol start="4">
<li>计算加权的Value：
将Value与softmax得到的概率分布相乘，得到加权后的Value，这是考虑了序列中其他元素的上下文信息的新表示。</li>
</ol>
<ul>
<li>加权Value = 概率分布 * V</li>
</ul>
<ol start="5">
<li>输出：
<strong>将加权Value相加，得到最终的输出，这是序列中每个元素的上下文表示。</strong></li>
</ol>
<ul>
<li>输出 = 加权Value之和
参数量的计算：</li>
<li>每个权重矩阵（Wq、Wk、Wv）的参数量为，因此总共有3个权重矩阵，参数量为。
为什么用多头（Multi-Head）注意力：</li>
<li>多头注意力允许模型在不同的表示子空间中学习信息，这样可以让模型同时关注不同的信息维度。每个头学习到的信息可以独立地编码输入序列的不同方面，然后将这些信息综合起来，得到更丰富的表示。
为什么要除以根号D：</li>
<li>将得分除以根号D（得分归一化）可以防止内积过大导致softmax函数梯度变得非常小，这有助于数值稳定性，使得学习过程更加稳定。此外，它还可以看作是一种缩放因子，帮助模型在不同维度上保持一致的性能。</li>
</ul>
<h3 id="自注意力机制code-dmoqv同源">自注意力机制code dmo（qv同源）</h3>
<p>假设我们有一个简单的句子：“猫喜欢追逐老鼠”。如果我们要对“喜欢”这个词进行编码，一个简单的方法是只看这个词本身，但这样会忽略它的上下文。“喜欢”的对象是“猫”，而被“喜欢”的是“追逐老鼠”。在这里，“猫”和“追逐老鼠”就是“喜欢”的上下文，而注意力机制能够帮助模型更好地捕获这种上下文关系。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="c1"># 使用PyTorch实现简单的点积注意力</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 初始化Query, Key, Value</span>
</span></span><span class="line"><span class="cl"><span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]])</span>  <span class="c1"># Query 对应于 &#34;喜欢&#34; 的编码</span>
</span></span><span class="line"><span class="cl"><span class="n">K</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">]])</span>  <span class="c1"># Key 对应于 &#34;猫&#34;, &#34;追逐&#34;, &#34;老鼠&#34; 的编码</span>
</span></span><span class="line"><span class="cl"><span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]])</span>  <span class="c1"># Value 也对应于 &#34;猫&#34;, &#34;追逐&#34;, &#34;老鼠&#34; 的编码</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 计算注意力权重</span>
</span></span><span class="line"><span class="cl"><span class="n">d_k</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">d_k</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 计算注意力输出</span>
</span></span><span class="line"><span class="cl"><span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">V</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;注意力权重:&#34;</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="s2">&#34;注意力输出:&#34;</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p>输出：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-lua" data-lang="lua"><span class="line"><span class="cl"><span class="err">注意力权重</span><span class="p">:</span> <span class="n">tensor</span><span class="p">(</span><span class="s">[[0.4761, 0.2678, 0.2561]]</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="err">注意力输出</span><span class="p">:</span> <span class="n">tensor</span><span class="p">(</span><span class="s">[[0.9529, 0.1797]]</span><span class="p">)</span>
</span></span></code></pre></td></tr></table>
</div>
</div><p><strong>这里，“喜欢”通过注意力权重与“猫”和“追逐老鼠”进行了信息的融合，并得到了一个新的编码，从而更准确地捕获了其在句子中的语义信息。</strong></p>
<p>通过这个例子，我们可以看到注意力机制是如何运作的，以及它在理解序列数据，特别是文本数据中的重要性</p>
<h3 id="multi-head-attention的对比">multi head Attention的对比</h3>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/f8c7d723c1ed2fe8ea842d3418ae6c60.png"
        data-srcset="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/f8c7d723c1ed2fe8ea842d3418ae6c60.png, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/f8c7d723c1ed2fe8ea842d3418ae6c60.png 1.5x, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/f8c7d723c1ed2fe8ea842d3418ae6c60.png 2x"
        data-sizes="auto"
        alt="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/f8c7d723c1ed2fe8ea842d3418ae6c60.png"
        title="refs/heads/master/image-20240818162519673" /></p>
<p><strong>Multi-Head Attention（多头注意力机制）本质上就是多个自注意力（Self-Attention）机制的并行应用，然后将它们的输出进行合并，并通过一个线性变换来得到最终的输出。</strong></p>
<p>使得模型能够更好地捕捉输入序列中的不同特征和关系。</p>
<h2 id="multi-query-attention-与-grouped-query-attention-是否了解区别是什么">Multi-query Attention 与 Grouped-query Attention 是否了解？区别是什么？</h2>
<h2 id="几种位置编码">几种位置编码</h2>
<p><strong>注意力机制则是位置不敏感的·，即使调换序列中两个元素的位置对编码后的结果也不会产生影响。</strong></p>
<p>因此，有必要将元素对应的位置信息添加到表示中，或者在计算注意力得分时考虑两个元素之间的相对位置。这些方法统称为位置编码，可以分为绝对位置编码和相对位置编码。</p>
<h3 id="绝对位置编码-absolute-position-encoding">绝对位置编码 Absolute Position Encoding</h3>
<p><strong>绝对位置编码</strong>是指在输入序列经过词嵌入后的第kk个<strong>token</strong>向量xk∈Rdxk∈Rd中加入(<strong>add</strong>)位置向量pk∈Rdpk∈Rd；其过程等价于首先向输入引入(<strong>concatenate</strong>)位置索引kk的<strong>one hot</strong>向量pk:xk+pkpk:xk+pk，再进行词嵌入；因此绝对位置编码也被称为<strong>位置嵌入(position embedding)</strong>。</p>
<h4 id="原理">原理</h4>
<ol>
<li>
<p><strong>生成位置编码</strong>：</p>
<ul>
<li>位置编码向量通常使用正弦和余弦函数生成，以确保不同位置的编码具有不同的特征。</li>
</ul>
</li>
<li>
<p><strong>加到输入嵌入上</strong>：</p>
<ul>
<li>
<p>生成的位置编码向量被直接加到输入嵌入上： [ X_{pos} = E_{pos} + PE_{pos} ] 其中，Epos<em>E<strong>p</strong>os</em> 是输入嵌入，PEpos<em>P<strong>E</strong>p**os</em> 是位置编码。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/35fa76f05396c65fd2904945da53d9f5.png"
        data-srcset="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/35fa76f05396c65fd2904945da53d9f5.png, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/35fa76f05396c65fd2904945da53d9f5.png 1.5x, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/35fa76f05396c65fd2904945da53d9f5.png 2x"
        data-sizes="auto"
        alt="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/35fa76f05396c65fd2904945da53d9f5.png"
        title="refs/heads/master/image-20240818165444253" /></p>
</li>
<li></li>
</ul>
</li>
</ol>
<h4 id="缺点">缺点</h4>
<ul>
<li><strong>位置依赖</strong>：绝对位置编码是基于具体位置的，因此在<strong>处理不同长度的序列时可能不够灵活</strong>。</li>
<li><strong>缺乏相对位置信息</strong>：绝对位置编码无法直接捕捉序列中元素之间的相对位置信息。</li>
</ul>
<h3 id="相对位置编码-relative-position-encoding">相对位置编码 Relative Position Encoding</h3>
<p>相对位置编码并不是直接建模每个输入<strong>token</strong>的位置信息，而是在计算注意力矩阵时考虑当前向量与待交互向量的位置的相对距离。</p>
<p>从绝对位置编码出发，其形式相当于在输入中添加入绝对位置的表示。</p>
<h4 id="原理-1">原理</h4>
<ol>
<li><strong>相对位置表示</strong>：
<ul>
<li>相对位置编码直接表示序列中元素之间的相对位置。例如，元素 i<em>i</em> 和元素 j<em>j</em> 之间的相对位置可以表示为 j−i<em>j</em>−<em>i</em>。</li>
<li>这种相对位置表示可以捕捉到序列中元素之间的相对关系，而不是具体的位置。</li>
</ul>
</li>
<li><strong>相对位置编码向量</strong>：
<ul>
<li>为每个相对位置生成一个编码向量，这些向量可以通过学习得到，或者使用类似绝对位置编码的方法生成。</li>
<li>在自注意力机制中，注意力权重的计算公式可以修改为： [ \text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T + QP^T}{\sqrt{d_k}}\right) V ] 其中，P<em>P</em> 是相对位置编码向量。</li>
</ul>
</li>
</ol>
<h4 id="优点">优点</h4>
<ul>
<li><strong>捕捉相对位置信息</strong>：相对位置编码能够直接捕捉序列中元素之间的相对位置信息，更加灵活。</li>
<li><strong>适应不同长度的序列</strong>：相对位置编码不依赖于具体位置，因此在处理不同长度的序列时更加灵活。</li>
</ul>
<h4 id="缺点-1">缺点</h4>
<ul>
<li><strong>计算开销</strong>：在某些情况下，相对位置编码可能会增加计算开销。</li>
</ul>
<h2 id="几种ffn前馈神经网络">几种ffn，前馈神经网络</h2>
<h3 id="为什么需要ffn">为什么需要ffn</h3>
<p><strong>Transformer中：其中两层感知机中，第一层会将输入的向量升维，第二层将向量重新降维。最后加个ReLU。这样子就可以学习到更加抽象的特征。（实现非线性变换，增强拟合能力）</strong></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/8c03b72687b7108fd8bd2d6778853385.png"
        data-srcset="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/8c03b72687b7108fd8bd2d6778853385.png, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/8c03b72687b7108fd8bd2d6778853385.png 1.5x, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/8c03b72687b7108fd8bd2d6778853385.png 2x"
        data-sizes="auto"
        alt="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/8c03b72687b7108fd8bd2d6778853385.png"
        title="img" /></p>
<h2 id="混合精度训练">混合精度训练</h2>
<h3 id="混合精度训练的基本原理">混合精度训练的基本原理</h3>
<p>混合精度训练的<strong>核心思想是将模型的某些部分（如前向传播和反向传播）使用低精度（FP16）计算，而其他关键部分（如权重更新）使用高精度（FP32）计算</strong>。具体来说，混合精度训练通常包括以下几个步骤：</p>
<ol>
<li><strong>模型参数和梯度的存储</strong>：模型的权重参数通常以FP32格式存储，以确保数值稳定性和精度。</li>
<li><strong>前向传播和反向传播</strong>：在前向传播和反向传播过程中，计算可以使用FP16格式，以加速计算和减少内存使用。</li>
<li><strong>损失缩放（Loss Scaling）</strong>：为了避免在低精度计算中出现数值下溢（underflow）问题，通常会对损失进行缩放。损失缩放的基本思想是将损失乘以一个缩放因子，使得梯度在反向传播过程中不会变得过小。</li>
<li><strong>权重更新</strong>：在权重更新阶段，<strong>梯度通常会被转换回FP32格式，并使用FP32格式的权重进行更新</strong>。</li>
</ol>
<h2 id="分布式训练dpmpddpppzero的三个stage">分布式训练dp，mp，ddp，pp；zero的三个stage</h2>
<h2 id="多模态clip">多模态clip</h2>
<h2 id="多模态的实现方式双流单流">多模态的实现方式（双流、单流）</h2>
<h1 id="vllm底层">VLLM底层</h1>
<h2 id="batch上如何优化">batch上如何优化</h2>
<p><strong>原始的朴素批处理方法</strong></p>
<p>处理完这一批后再开始下一批</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/5edaab8256d095636be685cf8aa82337.png"
        data-srcset="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/5edaab8256d095636be685cf8aa82337.png, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/5edaab8256d095636be685cf8aa82337.png 1.5x, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/5edaab8256d095636be685cf8aa82337.png 2x"
        data-sizes="auto"
        alt="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/5edaab8256d095636be685cf8aa82337.png"
        title="refs/heads/master/image-20240828102458058" /></p>
<p><strong>连续批处理</strong></p>
<p><strong>它采用了迭代级调度，其中批大小根据每次迭代确定。结果是，一旦批中的一个序列完成生成，就可以在其位置插入一个新的序列，从而实现比静态批处理更高的GPU利用率。</strong></p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/538568d012319c3ca4d7b3519bc15782.png"
        data-srcset="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/538568d012319c3ca4d7b3519bc15782.png, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/538568d012319c3ca4d7b3519bc15782.png 1.5x, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/538568d012319c3ca4d7b3519bc15782.png 2x"
        data-sizes="auto"
        alt="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/538568d012319c3ca4d7b3519bc15782.png"
        title="refs/heads/master/image-20240828102525386" /></p>
<p><strong>综上：处理请求分布差异越大，优化收益越高</strong></p>
<h2 id="预测推理speculative-inference">预测推理（<strong>Speculative inference</strong>）</h2>
<p>预测推理也称为推测采样、辅助生成或分块并行解码，是并行执行 LLM 的另一种方式。通常，GPT 风格的大语言模型是自回归模型，逐个生成文本标记。</p>
<p>生成的每个标记都依赖于它之前的所有标记来提供上下文。这意味着在常规执行中，<strong>不可能从同一个序列并行生成多个token，必须等待第 n 个token生成后才能生成 n+1 个token</strong>。</p>
<p>图 12 显示了预测推理的示例，其中临时模型临时预测并行验证或拒绝的多个未来步骤。在这种情况下，临时模型中的前两个预测token被接受，而最后一个在继续生成之前被拒绝并删除。</p>
<p><a href="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/02262415e08ac13dc032177dc17be642.png" target="_blank" rel="noopener noreffer "><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://dongnian.icu/note/llm/llm_concept/06.%E6%8E%A8%E7%90%86/llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/media/refs/heads/master/image_12.png"
        data-srcset="https://dongnian.icu/note/llm/llm_concept/06.%E6%8E%A8%E7%90%86/llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/media/refs/heads/master/image_12.png, https://dongnian.icu/note/llm/llm_concept/06.%E6%8E%A8%E7%90%86/llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/media/refs/heads/master/image_12.png 1.5x, https://dongnian.icu/note/llm/llm_concept/06.%E6%8E%A8%E7%90%86/llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/media/refs/heads/master/image_12.png 2x"
        data-sizes="auto"
        alt="https://dongnian.icu/note/llm/llm_concept/06.%E6%8E%A8%E7%90%86/llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/media/refs/heads/master/image_12.png"
        title="img" /></a></p>
<h2 id="prefix-caching-优化">prefix Caching 优化</h2>
<p>输入的头部由于system prompt等因素，相似新高，可以考虑缓存相关计算结果，避免重复计算和显存浪费</p>
<ul>
<li>batch内的复用</li>
<li>batch之间的复用</li>
</ul>
<h2 id="kv-cache">KV-Cache</h2>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/28e3bf634a58e3512135107171df837d.png"
        data-srcset="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/28e3bf634a58e3512135107171df837d.png, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/28e3bf634a58e3512135107171df837d.png 1.5x, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/28e3bf634a58e3512135107171df837d.png 2x"
        data-sizes="auto"
        alt="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/28e3bf634a58e3512135107171df837d.png"
        title="img" /></p>
<h2 id="训练加速---pipeline并行">训练加速&mdash;Pipeline并行</h2>
<p>（b）Pipeline并行化<strong>将模型（垂直）分片为块，其中每个块包含在单独设备上执行的层的子集</strong>。图 2a 说明了四路Pipeline，其中模型按顺序分区，并且所有层的四分之一子集在每个设备上执行。一个设备上的一组操作的输出被传递到下一个设备，后者继续执行后续块。Fn<em>F**n</em>和 Bn<em>B**n</em>分别表示设备 n<em>n</em> 上的前向传播和后向传播。每个设备上存储模型权重的内存需求被分成四份。</p>
<p>只要分的够细，就会看图C，通过微批处理</p>
<p>（c）<strong>微批处理可以在一定程度上缓解这种情况</strong>，如图 2c 所示。输入的全局批次大小被分成子批次，这些子批次被一一处理，最后累积梯度。请注意，Fn,m<em>F**n</em>,<em>m</em> 和 Bn,m<em>B**n</em>,<em>m</em> 分别表示设备<code>n</code>上<code>m</code>批次的前向和后向传递。<strong>这种方法缩小了管道气泡的尺寸，但并没有完全消除它们</strong>。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/13e05a79d624035eddbdba8840412298.png"
        data-srcset="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/13e05a79d624035eddbdba8840412298.png, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/13e05a79d624035eddbdba8840412298.png 1.5x, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/13e05a79d624035eddbdba8840412298.png 2x"
        data-sizes="auto"
        alt="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/13e05a79d624035eddbdba8840412298.png"
        title="img" /></p>
<h2 id="训练加速---tensor并行相当于每一层直接切分为多个并行">训练加速&mdash;Tensor并行（相当于每一层，直接切分为多个并行）</h2>
<p>Tensor并行化<strong>将模型的各个层（水平）分片为更小的、独立的计算块，这些计算块可以在不同的设备上执行</strong>。Transformer的主要组成部分，注意力块和多层感知器（MLP）层是可以利用Tensor并行化的。在多头注意力块中，每个头或一组头可以分配给不同的设备，以便它们可以独立且并行地计算。</p>
<p><img
        class="lazyload"
        src="/svg/loading.min.svg"
        data-src="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/8fb5185ef70f6cc077349f1b10116b2a.png"
        data-srcset="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/8fb5185ef70f6cc077349f1b10116b2a.png, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/8fb5185ef70f6cc077349f1b10116b2a.png 1.5x, https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/8fb5185ef70f6cc077349f1b10116b2a.png 2x"
        data-sizes="auto"
        alt="https://raw.githubusercontent.com/kengerlwl/kengerlwl.github.io/refs/heads/master/image/1f448b2cd4b86e188b8b0eb83c9847b2/8fb5185ef70f6cc077349f1b10116b2a.png"
        title="在这里插入图片描述" /></p>
<p><strong>Tensor并行化是有局限性，它需要将层划分为独立的、可管理的块，不适用于 <code>LayerNorm </code>和 <code>Dropout </code>等操作，而是在tensor并行中复制。</strong></p>
<p><a href="https://dongnian.icu/note/llm/llm_concept/06.%E6%8E%A8%E7%90%86/06.%E6%8E%A8%E7%90%86.html" target="_blank" rel="noopener noreffer ">37.2° Blog | 37.2° Blog</a></p>
</div><div class="post-footer" id="post-footer">
    <div class="post-info">
        <div class="post-info-line">
            <div class="post-info-mod">
                <span>更新于 2024-08-18</span>
            </div></div>
        <div class="post-info-line">
            <div class="post-info-md"><span>
                            <a class="link-to-markdown" href="/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E5%85%AB%E8%82%A1/index.md" target="_blank">阅读原始文档</a>
                        </span></div>
            <div class="post-info-share">
                <span><a href="javascript:void(0);" title="分享到 Twitter" data-sharer="twitter" data-url="https://kengerlwl.github.io/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E5%85%AB%E8%82%A1/" data-title="大模型底层八股" data-hashtags="pytorch,Transformer,LLM"><i class="fab fa-twitter fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Facebook" data-sharer="facebook" data-url="https://kengerlwl.github.io/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E5%85%AB%E8%82%A1/" data-hashtag="pytorch"><i class="fab fa-facebook-square fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Hacker News" data-sharer="hackernews" data-url="https://kengerlwl.github.io/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E5%85%AB%E8%82%A1/" data-title="大模型底层八股"><i class="fab fa-hacker-news fa-fw" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 Line" data-sharer="line" data-url="https://kengerlwl.github.io/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E5%85%AB%E8%82%A1/" data-title="大模型底层八股"><i data-svg-src="https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg" aria-hidden="true"></i></a><a href="javascript:void(0);" title="分享到 微博" data-sharer="weibo" data-url="https://kengerlwl.github.io/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%BA%95%E5%B1%82%E5%85%AB%E8%82%A1/" data-title="大模型底层八股"><i class="fab fa-weibo fa-fw" aria-hidden="true"></i></a></span>
            </div>
        </div>
    </div>

    <div class="post-info-more">
        <section class="post-tags"><i class="fas fa-tags fa-fw" aria-hidden="true"></i>&nbsp;<a href="/tags/pytorch/">Pytorch</a>,&nbsp;<a href="/tags/transformer/">Transformer</a>,&nbsp;<a href="/tags/llm/">Llm</a></section>
        <section>
            <span><a href="javascript:void(0);" onclick="window.history.back();">返回</a></span>&nbsp;|&nbsp;<span><a href="/">主页</a></span>
        </section>
    </div>

    <div class="post-nav"><a href="/%E5%A4%A7%E6%A8%A1%E5%9E%8Btoken%E5%8E%8B%E7%BC%A9/" class="prev" rel="prev" title="大模型token压缩"><i class="fas fa-angle-left fa-fw" aria-hidden="true"></i>大模型token压缩</a>
            <a href="/python%E5%85%AB%E8%82%A1%E9%A2%98/" class="next" rel="next" title="python八股题">python八股题<i class="fas fa-angle-right fa-fw" aria-hidden="true"></i></a></div>
</div>
<div id="comments"><div id="giscus" class="comment"></div><noscript>
                Please enable JavaScript to view the comments powered by <a href="https://giscus.app">Giscus</a>.
            </noscript></div></article></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">由 <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.132.1">Hugo</a> 强力驱动 | 主题 - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.11"><i class="far fa-kiss-wink-heart fa-fw" aria-hidden="true"></i> LoveIt</a>
                </div><div class="footer-line" itemscope itemtype="http://schema.org/CreativeWork"><i class="far fa-copyright fa-fw" aria-hidden="true"></i><span itemprop="copyrightYear">2019 - 2026</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="https://kengerlwl.github.io/" target="_blank">kenger</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="回到顶部">
                <i class="fas fa-arrow-up fa-fw" aria-hidden="true"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="查看评论">
                <i class="fas fa-comment fa-fw" aria-hidden="true"></i>
            </a>
        </div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js"></script><script type="text/javascript">window.config={"code":{"copyTitle":"复制到剪贴板","maxShownLines":50},"comment":{"giscus":{"category":"Announcements","categoryId":"DIC_kwDOFA4dBM4Ce_0B","darkTheme":"github-dark","emitMetadata":"0","inputPosition":"bottom","lang":"zh-CN","lazyLoading":false,"lightTheme":"github-light","mapping":"pathname","reactionsEnabled":"1","repo":"kengerlwl/kengerlwl.github.io","repoId":"MDEwOlJlcG9zaXRvcnkzMzY0NjkyNTI="}},"search":{"algoliaAppID":"JCTYUNKA9R","algoliaIndex":"kenger","algoliaSearchKey":"3ef68664495033362b7df9cf5a3eee1e","highlightTag":"em","maxResultLength":10,"noResultsFound":"没有找到结果","snippetLength":50,"type":"algolia"}};</script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
